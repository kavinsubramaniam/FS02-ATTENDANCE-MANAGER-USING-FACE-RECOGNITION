{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a **highly efficient and fast face recognition system**, you need a balance of **accuracy, speed, and resource efficiency**. Below is an **advanced approach** using state-of-the-art methods:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Model Selection (High Speed + Accuracy)**\n",
    "#### **a. Efficient CNN-based Models**\n",
    "- **MobileFaceNet** â€“ Lightweight and optimized for mobile/edge devices.\n",
    "- **EfficientNet-based Face Recognition** â€“ Balances accuracy and speed.\n",
    "- **LightCNN** â€“ Low computational cost with good accuracy.\n",
    "\n",
    "#### **b. Transformer-based Models (Advanced)**\n",
    "- **FaceViT (Vision Transformers for Faces)** â€“ Higher accuracy but slightly slower.\n",
    "- **MobileViT** â€“ Efficient transformer-based face recognition.\n",
    "\n",
    "#### **c. Hybrid Models**\n",
    "- **SqueezeFaceNet (CNN + Transformer)** â€“ Combines CNN efficiency with transformer robustness.\n",
    "- **Attention-based FaceNet** â€“ Uses self-attention for better feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Face Embedding and Feature Extraction**\n",
    "- **ArcFace, CosFace, SphereFace** â€“ Advanced loss functions for **better discriminability**.\n",
    "- **InsightFace** â€“ A state-of-the-art framework implementing ArcFace with ResNet.\n",
    "- **GhostFaceNet** â€“ Lightweight model optimized for IoT and edge devices.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Optimization for Speed**\n",
    "- **Model Quantization** â€“ Use **FP16/INT8 quantization** with TensorRT or TFLite to speed up inference.\n",
    "- **Knowledge Distillation** â€“ Train a smaller model using a larger one (e.g., distilling ArcFace into MobileFaceNet).\n",
    "- **ONNX Runtime** â€“ Optimize inference for multiple hardware platforms.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Face Detection & Alignment (Preprocessing)**\n",
    "- **Fastest Detectors**:\n",
    "  - **YOLO-Face** â€“ Real-time face detection.\n",
    "  - **RetinaFace** â€“ High accuracy with landmarks.\n",
    "  - **MTCNN (Multitask Cascaded CNN)** â€“ Good for face alignment.\n",
    "\n",
    "- **Alignment Techniques**:\n",
    "  - **5-point/68-point landmark-based alignment** before feature extraction.\n",
    "  - **Normalization using Procrustes Analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Deployment Optimization**\n",
    "- **CUDA + TensorRT** â€“ Accelerate inference on **NVIDIA GPUs**.\n",
    "- **OpenVINO** â€“ Optimize for **Intel CPUs and VPUs**.\n",
    "- **Edge TPU (Google Coral) or Jetson Nano** â€“ Hardware acceleration for embedded systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. System Architecture for Real-time Processing**\n",
    "- **Parallel Processing** â€“ Use **multi-threading** or **async GPU inference**.\n",
    "- **Face Embedding Caching** â€“ Store embeddings in RAM for quick retrieval.\n",
    "- **Database Optimization** â€“ Use **FAISS (Facebook AI Similarity Search)** for fast nearest neighbor searches.\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Frameworks for Implementation**\n",
    "- **DeepFace** â€“ Pre-built high-performance models (ArcFace, FaceNet, Dlib).\n",
    "- **InsightFace** â€“ State-of-the-art library optimized for real-world applications.\n",
    "- **FaceNet (TensorFlow/PyTorch)** â€“ Robust and widely used.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Recommendation**\n",
    "For **real-time, efficient, and scalable face recognition**, use:\n",
    "âœ… **ArcFace on MobileFaceNet** (Fast and accurate).  \n",
    "âœ… **YOLO-Face + RetinaFace for detection** (Lightweight and fast).  \n",
    "âœ… **TensorRT or OpenVINO** for deployment optimizations.  \n",
    "âœ… **FAISS for fast face searches** in large databases.\n",
    "\n",
    "Would you like a complete **implementation pipeline** for a specific platform (e.g., cloud, edge, mobile)? ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a **step-by-step breakdown** of an **advanced face recognition system** with **state-of-the-art algorithms** and **methods** for each stage.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Data Collection & Preprocessing**  \n",
    "### **(A) Data Collection**  \n",
    "- **MS-Celeb-1M, VGGFace2, CASIA-WebFace, MegaFace** â€“ Large-scale datasets for training.  \n",
    "- **Synthetic Data Augmentation** â€“ GAN-based methods (StyleGAN, StarGAN, FaceShifter).  \n",
    "\n",
    "### **(B) Face Detection & Alignment**  \n",
    "- **YOLO-Face, RetinaFace, CenterFace, DSFD** â€“ Fast and accurate face detection.  \n",
    "- **MTCNN, 3D Dense Face Alignment, Super-FAN** â€“ Face alignment for pose correction.  \n",
    "- **Procrustes Analysis, Thin Plate Spline (TPS)** â€“ Geometric alignment.  \n",
    "\n",
    "### **(C) Data Augmentation & Normalization**  \n",
    "- **Cutout, MixUp, CutMix** â€“ Robust augmentation for better generalization.  \n",
    "- **Histogram Equalization, CLAHE, Gamma Correction** â€“ Contrast enhancement.  \n",
    "- **Whitening, Mean-Variance Normalization** â€“ Standardizing input.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Feature Extraction & Embedding Learning**  \n",
    "### **(A) Backbone Architectures**  \n",
    "- **MobileFaceNet, GhostNet, EfficientNet-Face, LightCNN** â€“ Lightweight feature extractors.  \n",
    "- **ViT, Swin Transformer, MobileViT** â€“ Transformer-based deep face models.  \n",
    "- **ResNet-100, ResNeXt, SENet** â€“ High-performance CNN architectures.  \n",
    "\n",
    "### **(B) Loss Functions (Face Embeddings Optimization)**  \n",
    "- **ArcFace, CosFace, SphereFace, SubCenter ArcFace** â€“ Margin-based loss functions for better discrimination.  \n",
    "- **ElasticFace, AdaFace** â€“ Adaptive loss for imbalanced data.  \n",
    "- **Contrastive Loss, Triplet Loss** â€“ Distance-based loss functions.  \n",
    "\n",
    "### **(C) Feature Representation Techniques**  \n",
    "- **High-Dimensional Face Embeddings (512D, 1024D)** â€“ High-quality representations.  \n",
    "- **Metric Learning (Euclidean, Angular, Mahalanobis distance)** â€“ Similarity scoring.  \n",
    "- **Principal Component Analysis (PCA), LDA** â€“ Dimensionality reduction.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Model Optimization & Compression**  \n",
    "### **(A) Acceleration Techniques**  \n",
    "- **TensorRT, ONNX Runtime, OpenVINO** â€“ Hardware-accelerated inference.  \n",
    "- **CUDA & cuDNN Optimization** â€“ GPU parallel computing.  \n",
    "- **Multi-threading & Asynchronous Processing** â€“ Efficient workload distribution.  \n",
    "\n",
    "### **(B) Model Compression**  \n",
    "- **Quantization (INT8, FP16, BinaryNet, TernaryNet)** â€“ Reduce model size.  \n",
    "- **Pruning (Structured & Unstructured)** â€“ Remove redundant parameters.  \n",
    "- **Knowledge Distillation (TinyFaceNet, MobileFaceDistill)** â€“ Transfer knowledge to a smaller model.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Face Recognition Pipeline (Inference Phase)**  \n",
    "### **(A) Face Matching & Similarity Search**  \n",
    "- **FAISS (Facebook AI Similarity Search)** â€“ Fast nearest neighbor search.  \n",
    "- **HNSW (Hierarchical Navigable Small World Graphs)** â€“ Scalable indexing.  \n",
    "- **KD-Trees, Ball Trees** â€“ Efficient search structures.  \n",
    "\n",
    "### **(B) Face Anti-Spoofing & Liveness Detection**  \n",
    "- **CDC, DepthNet, FaceDe-Spoofing GAN** â€“ Detect fake faces.  \n",
    "- **Multi-modal Anti-Spoofing (RGB + IR + Depth)** â€“ High-security face verification.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Model Deployment & Scalability**  \n",
    "### **(A) Edge & Cloud Deployment**  \n",
    "- **Jetson Nano, Edge TPU (Google Coral), RKNN** â€“ Efficient edge deployment.  \n",
    "- **AWS Lambda, Azure ML, GCP AI Platform** â€“ Cloud-based scalability.  \n",
    "- **Kubernetes, Docker, FastAPI, Flask** â€“ Containerized deployment.  \n",
    "\n",
    "### **(B) Security & Privacy Enhancements**  \n",
    "- **Homomorphic Encryption for Face Data** â€“ Privacy-preserving computation.  \n",
    "- **Differential Privacy & Federated Learning (FATE, Flower)** â€“ Secure distributed learning.  \n",
    "- **Blockchain-based Identity Verification** â€“ Immutable facial authentication.  \n",
    "\n",
    "---\n",
    "\n",
    "This is a **highly advanced and efficient approach** covering **everything from data to deployment**. Do you need an **implementation roadmap** for any of these steps? ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.80-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (1.15.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (0.21.0+cu124)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (6.1.0)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pandas>=1.1.4 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from torch>=1.8.0->ultralytics) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Downloading ultralytics-8.3.80-py3-none-any.whl (921 kB)\n",
      "   ---------------------------------------- 0.0/921.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/921.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/921.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/921.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/921.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/921.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/921.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/921.9 kB ? eta -:--:--\n",
      "   --------------------- ---------------- 524.3/921.9 kB 409.0 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 524.3/921.9 kB 409.0 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 524.3/921.9 kB 409.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 786.4/921.9 kB 394.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 921.9/921.9 kB 422.1 kB/s eta 0:00:00\n",
      "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo, ultralytics-thop, ultralytics\n",
      "Successfully installed py-cpuinfo-9.0.0 ultralytics-8.3.80 ultralytics-thop-2.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install deepface\n",
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv11-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.80  Python-3.12.8 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov11n-face.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (5.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('./yolov11n-face.pt')\n",
    "model.export(format=\"engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 A:\\PROJECTS\\FULL_STACK_PROJECTS\\FS02-ATTENDANCE-MANAGER-USING-FACE-RECOGNITION\\model\\..\\data\\images\\val\\0be97b86c8baf73f.jpg: 448x640 1 face, 137.8ms\n",
      "Speed: 4.1ms preprocess, 137.8ms inference, 154.7ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'face'}\n",
       " obb: None\n",
       " orig_img: array([[[30, 43, 51],\n",
       "         [30, 43, 51],\n",
       "         [30, 43, 51],\n",
       "         ...,\n",
       "         [66, 66, 66],\n",
       "         [66, 66, 66],\n",
       "         [66, 66, 66]],\n",
       " \n",
       "        [[30, 43, 51],\n",
       "         [30, 43, 51],\n",
       "         [30, 43, 51],\n",
       "         ...,\n",
       "         [66, 66, 66],\n",
       "         [66, 66, 66],\n",
       "         [66, 66, 66]],\n",
       " \n",
       "        [[30, 43, 51],\n",
       "         [30, 43, 51],\n",
       "         [30, 43, 51],\n",
       "         ...,\n",
       "         [66, 66, 66],\n",
       "         [66, 66, 66],\n",
       "         [66, 66, 66]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[40, 42, 52],\n",
       "         [39, 41, 51],\n",
       "         [39, 41, 51],\n",
       "         ...,\n",
       "         [35, 40, 41],\n",
       "         [36, 41, 42],\n",
       "         [36, 41, 42]],\n",
       " \n",
       "        [[40, 42, 52],\n",
       "         [39, 41, 51],\n",
       "         [39, 41, 51],\n",
       "         ...,\n",
       "         [34, 39, 40],\n",
       "         [35, 40, 41],\n",
       "         [35, 40, 41]],\n",
       " \n",
       "        [[39, 41, 51],\n",
       "         [39, 41, 51],\n",
       "         [39, 41, 51],\n",
       "         ...,\n",
       "         [33, 38, 39],\n",
       "         [33, 38, 39],\n",
       "         [34, 39, 40]]], dtype=uint8)\n",
       " orig_shape: (683, 1024)\n",
       " path: 'A:\\\\PROJECTS\\\\FULL_STACK_PROJECTS\\\\FS02-ATTENDANCE-MANAGER-USING-FACE-RECOGNITION\\\\model\\\\..\\\\data\\\\images\\\\val\\\\0be97b86c8baf73f.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 4.0569999982835725, 'inference': 137.82570000330452, 'postprocess': 154.68380000675097}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"../data/images/val/0be97b86c8baf73f.jpg\", save=True, project='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in a:\\software\\applications\\python\\python3.12\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.7 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 987.4 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.7 MB 949.8 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install retina-face\n",
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From A:\\SOFTWARE\\APPLICATIONS\\Python\\python3.12\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from retinaface import RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mRetinaFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/images/val/0be97b86c8baf73f.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\SOFTWARE\\APPLICATIONS\\Python\\python3.12\\Lib\\site-packages\\retinaface\\RetinaFace.py:96\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m    100\u001b[0m nms_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n",
      "File \u001b[1;32mA:\\SOFTWARE\\APPLICATIONS\\Python\\python3.12\\Lib\\site-packages\\retinaface\\RetinaFace.py:54\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m model  \u001b[38;5;66;03m# singleton design pattern\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m     53\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m---> 54\u001b[0m         \u001b[43mretinaface_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     55\u001b[0m         input_signature\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m3\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),),\n\u001b[0;32m     56\u001b[0m     )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mA:\\SOFTWARE\\APPLICATIONS\\Python\\python3.12\\Lib\\site-packages\\retinaface\\model\\retinaface_model.py:1027\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1019\u001b[0m ssh_m3_det_conv1_bn \u001b[38;5;241m=\u001b[39m BatchNormalization(\n\u001b[0;32m   1020\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.9999999494757503e-05\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssh_m3_det_conv1_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m )(ssh_m3_det_conv1)\n\u001b[0;32m   1023\u001b[0m ssh_m3_det_context_conv1_bn \u001b[38;5;241m=\u001b[39m BatchNormalization(\n\u001b[0;32m   1024\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.9999999494757503e-05\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssh_m3_det_context_conv1_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m )(ssh_m3_det_context_conv1)\n\u001b[1;32m-> 1027\u001b[0m x1_shape \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssh_c3_up\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1028\u001b[0m x2_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(ssh_c2_lateral_relu)\n\u001b[0;32m   1029\u001b[0m offsets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, (x1_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x2_shape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, (x1_shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m x2_shape[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mA:\\SOFTWARE\\APPLICATIONS\\Python\\python3.12\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mA:\\SOFTWARE\\APPLICATIONS\\Python\\python3.12\\Lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "# resp = RetinaFace.detect_faces(\"../data/images/val/0be97b86c8baf73f.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deepface' has no attribute 'represent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mdeepface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprojects/face_0.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'deepface' has no attribute 'represent'"
     ]
    }
   ],
   "source": [
    "image = deepface.represent('projects/face_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
